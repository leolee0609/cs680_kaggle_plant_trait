{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "seed = 42\n",
      "num_workers = 4\n",
      "batch_size = 32\n",
      "model_name = vit_small_patch14_reg4_dinov2.lvd142m\n",
      "img_size = 224\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_df.shape = (43363, 170)\n",
      "test_df.shape = (6391, 164)\n",
      "device = device(type='cuda')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/br/.cache/torch/hub/facebookresearch_dinov2_main\n",
      "/home/br/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/swiglu_ffn.py:43: UserWarning: xFormers is available (SwiGLU)\n",
      "  warnings.warn(\"xFormers is available (SwiGLU)\")\n",
      "/home/br/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:27: UserWarning: xFormers is available (Attention)\n",
      "  warnings.warn(\"xFormers is available (Attention)\")\n",
      "/home/br/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/block.py:33: UserWarning: xFormers is available (Block)\n",
      "  warnings.warn(\"xFormers is available (Block)\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DinoVisionTransformer(\n",
       "  (patch_embed): PatchEmbed(\n",
       "    (proj): Conv2d(3, 1536, kernel_size=(14, 14), stride=(14, 14))\n",
       "    (norm): Identity()\n",
       "  )\n",
       "  (blocks): ModuleList(\n",
       "    (0-39): 40 x NestedTensorBlock(\n",
       "      (norm1): LayerNorm((1536,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): MemEffAttention(\n",
       "        (qkv): Linear(in_features=1536, out_features=4608, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): LayerScale()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((1536,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): SwiGLUFFNFused(\n",
       "        (w12): Linear(in_features=1536, out_features=8192, bias=True)\n",
       "        (w3): Linear(in_features=4096, out_features=1536, bias=True)\n",
       "      )\n",
       "      (ls2): LayerScale()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "  )\n",
       "  (norm): LayerNorm((1536,), eps=1e-06, elementwise_affine=True)\n",
       "  (head): Identity()\n",
       ")"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import timm\n",
    "import pandas as pd\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "import albumentations as A\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import r2_score\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts, OneCycleLR, CosineAnnealingLR, ReduceLROnPlateau, StepLR, LambdaLR\n",
    "\n",
    "from utils import seed_torch, current_date_time, init_logger\n",
    "\n",
    "# os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "os.environ[\"HTTP_PROXY\"] = \"http://127.0.0.1:7890\"\n",
    "os.environ[\"HTTPS_PROXY\"] = \"http://127.0.0.1:7890\"\n",
    "\n",
    "\n",
    "class CFG:\n",
    "    seed = 42\n",
    "    num_workers = 4\n",
    "\n",
    "    batch_size = 32\n",
    "    \n",
    "    model_name = 'vit_small_patch14_reg4_dinov2.lvd142m' # efficientnet_b0 swin_large_patch4_window7_224 swin_tiny_patch4_window7_224 vit_giant_patch14_reg4_dinov2.lvd142m vit_small_patch14_reg4_dinov2.lvd142m\n",
    "    img_size = 224 # 128 224 518\n",
    "\n",
    "   \n",
    "seed_torch(CFG.seed)\n",
    "cur_time = current_date_time()\n",
    "cur_time_abbr = cur_time.replace(\"-\", \"\").replace(\":\", \"\").replace(\" \", \"\")[4:12]\n",
    "output_dir = './output'\n",
    "output_dir = f\"{output_dir}/{cur_time_abbr}_get_embedding\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "LOGGER = init_logger(f'{output_dir}/get_embedding.log')\n",
    "\n",
    "for key, value in CFG.__dict__.items():\n",
    "    if not key.startswith(\"__\"):\n",
    "        LOGGER.info(f\"{key} = {value}\")\n",
    "\n",
    "\n",
    "# 读取数据\n",
    "train_df = pd.read_csv('./data/train.csv')\n",
    "test_df = pd.read_csv('./data/test.csv')\n",
    "\n",
    "train_df['file_path'] = train_df['id'].apply(lambda x: f'./data/train_images/{x}.jpeg')\n",
    "test_df['file_path'] = test_df['id'].apply(lambda x: f'./data/test_images/test_images/{x}.jpeg')\n",
    "\n",
    "print(f\"{train_df.shape = }\")\n",
    "print(f\"{test_df.shape = }\")\n",
    "\n",
    "# 设置设备\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"{device = }\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.hub.load('facebookresearch/dinov2', 'dinov2_vitg14_reg').to(device)\n",
    "model.eval()\n",
    "\n",
    "# 数据增强和预处理\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(CFG.img_size, interpolation=3),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image_embeddings_dino(model, preprocess, batch_size, df):\n",
    "    image_embeddings = []\n",
    "    for i in tqdm(range(0, len(df), batch_size)):\n",
    "        paths = df['file_path'][i:i + batch_size]\n",
    "        image_tensor = torch.stack([preprocess(Image.open(path)) for path in paths]).to(device)\n",
    "        with torch.no_grad():\n",
    "            curr_image_embeddings = model(image_tensor)\n",
    "        image_embeddings.extend(curr_image_embeddings.cpu().numpy())\n",
    "    return image_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1356/1356 [29:25<00:00,  1.30s/it]\n"
     ]
    }
   ],
   "source": [
    "train_image_embeddings = get_image_embeddings_dino(model, transform, CFG.batch_size, train_df)\n",
    "np.save(f'train_dinov2_embeds', np.array(train_image_embeddings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [04:15<00:00,  1.28s/it]\n"
     ]
    }
   ],
   "source": [
    "test_image_embeddings = get_image_embeddings_dino(model, transform, CFG.batch_size, test_df)\n",
    "np.save(f'test_dinov2_embeds', np.array(test_image_embeddings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
