{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "n_fold = 10\n",
      "seed = 44\n",
      "num_workers = 4\n",
      "target_cols = ['X4_mean', 'X11_mean', 'X18_mean', 'X26_mean', 'X50_mean', 'X3112_mean']\n",
      "sub_cols = ['X4', 'X11', 'X18', 'X26', 'X50', 'X3112']\n",
      "fold_list = [0]\n",
      "full_data_train = True\n",
      "use_clip = True\n",
      "clip_min = 0.001\n",
      "clip_max = 0.99\n",
      "use_label_norm = False\n",
      "use_log10 = False\n",
      "label_norm_method = minmax\n",
      "use_feature_norm = True\n",
      "first_n_poly_feats = 1000\n",
      "lr = 0.06\n",
      "iterations = 1500\n",
      "num_outputs = 6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device = device(type='cuda')\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import os\n",
    "import torch\n",
    "import timm\n",
    "import pandas as pd\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "import albumentations as A\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import r2_score\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts, OneCycleLR, CosineAnnealingLR, ReduceLROnPlateau, StepLR, LambdaLR\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score\n",
    "from catboost import Pool, CatBoostRegressor\n",
    "from torchvision import transforms\n",
    "\n",
    "from utils import seed_torch, current_date_time, init_logger\n",
    "\n",
    "# os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "# os.environ[\"HTTP_PROXY\"] = \"http://127.0.0.1:7890\"\n",
    "# os.environ[\"HTTPS_PROXY\"] = \"http://127.0.0.1:7890\"\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"{device = }\")\n",
    "\n",
    "class CFG:\n",
    "    n_fold = 10\n",
    "    seed = 44\n",
    "    num_workers = 4\n",
    "    target_cols = ['X4_mean', 'X11_mean', 'X18_mean', 'X26_mean', 'X50_mean', 'X3112_mean']\n",
    "    sub_cols = ['X4', 'X11', 'X18', 'X26', 'X50', 'X3112']\n",
    "    fold_list = [0]\n",
    "    full_data_train = True\n",
    "\n",
    "    use_clip = True\n",
    "    clip_min = 0.001 # 0.001\n",
    "    clip_max = 0.990 # 0.990\n",
    "    \n",
    "    use_label_norm = False\n",
    "    use_log10 = False\n",
    "    label_norm_method = 'minmax' # minmax zscore\n",
    "    \n",
    "    use_feature_norm = True\n",
    "    first_n_poly_feats = 1000\n",
    "\n",
    "    lr = 0.06 # 0.06\n",
    "    iterations = 1500 # 1500\n",
    "\n",
    "CFG.num_outputs = len(CFG.target_cols)\n",
    "\n",
    "seed_torch(CFG.seed)\n",
    "cur_time = current_date_time()\n",
    "cur_time_abbr = cur_time.replace(\"-\", \"\").replace(\":\", \"\").replace(\" \", \"\")[4:12]\n",
    "output_dir = './output'\n",
    "output_dir = f\"{output_dir}/{cur_time_abbr}_catboost\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "LOGGER = init_logger(f'{output_dir}/train_catboost.log')\n",
    "\n",
    "for key, value in CFG.__dict__.items():\n",
    "    if not key.startswith(\"__\"):\n",
    "        LOGGER.info(f\"{key} = {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_df.shape = (43363, 171)\n",
      "test_df.shape = (6391, 165)\n",
      "train_image_embeddings.shape = (43363, 1536)\n",
      "test_image_embeddings.shape = (6391, 1536)\n"
     ]
    }
   ],
   "source": [
    "# 读取数据\n",
    "train_df = pd.read_csv('./data/train.csv')\n",
    "test_df = pd.read_csv('./data/test.csv')\n",
    "\n",
    "train_df['file_path'] = train_df['id'].apply(lambda x: f'./data/train_images/{x}.jpeg')\n",
    "test_df['file_path'] = test_df['id'].apply(lambda x: f'./data/test_images/test_images/{x}.jpeg')\n",
    "\n",
    "print(f\"{train_df.shape = }\")\n",
    "print(f\"{test_df.shape = }\")\n",
    "\n",
    "# need run get_embedding.ipynb first\n",
    "train_image_embeddings = np.load(f'train_dinov2_embeds.npy')\n",
    "test_image_embeddings = np.load(f'test_dinov2_embeds.npy')\n",
    "print(f\"{train_image_embeddings.shape = }\")\n",
    "print(f\"{test_image_embeddings.shape = }\")\n",
    "\n",
    "\n",
    "# 删除列\n",
    "drop_cols = ['id', 'file_path'] + CFG.target_cols\n",
    "# 特征列\n",
    "feature_cols = [col for col in train_df.columns if col not in drop_cols]\n",
    "# 5-fold交叉验证\n",
    "kf = KFold(n_splits=CFG.n_fold, shuffle=True, random_state=CFG.seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_norm(df, use_log10, norm_method, minmax_dict=None):\n",
    "    if use_log10:\n",
    "        # 对6标签做 log10 处理\n",
    "        for col in CFG.target_cols:\n",
    "            df[col] = np.log10(df[col] + 1)\n",
    "            print(f\"log10 {col}: {df[col].min() = }, {df[col].max() = }\")\n",
    "    \n",
    "    if norm_method == 'minmax':\n",
    "        # ['X4_mean', 'X11_mean', 'X18_mean', 'X26_mean', 'X50_mean', 'X3112_mean'] 的最大值和最小值\n",
    "        if minmax_dict is None:\n",
    "            minmax_dict = {}\n",
    "            for col in CFG.target_cols:\n",
    "                minmax_dict[col] = {}\n",
    "                minmax_dict[col]['min'] = df[col].min()\n",
    "                minmax_dict[col]['max'] = df[col].max()\n",
    "            print(f\"{minmax_dict = }\")\n",
    "\n",
    "        # 对6列标签进行归一化\n",
    "        for col in CFG.target_cols:\n",
    "            df[col] = (df[col] - minmax_dict[col]['min']) / (minmax_dict[col]['max'] - minmax_dict[col]['min'])\n",
    "            print(f\"minmax {col}: {df[col].min() = }, {df[col].max() = }\")\n",
    "\n",
    "\n",
    "    return df, minmax_dict\n",
    "\n",
    "\n",
    "def feature_norm(df, feature_cols, scaler):\n",
    "    if scaler is None:\n",
    "        scaler = StandardScaler()\n",
    "        df[feature_cols] = scaler.fit_transform(df[feature_cols]).astype(np.float32)\n",
    "    else:\n",
    "        df[feature_cols] = scaler.transform(df[feature_cols]).astype(np.float32)\n",
    "    return df, scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fold 1/10\n",
      "train_subset.shape = (43363, 171), val_subset.shape = (4337, 171)\n",
      "after clip, train_subset.shape = (40587, 171), val_subset.shape = (4337, 171)\n",
      "/tmp/ipykernel_75282/2205508010.py:32: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[feature_cols] = scaler.transform(df[feature_cols]).astype(np.float32)\n",
      "train_features_mask_df.shape = (40587, 2537), val_features_mask_df.shape = (4337, 2537)\n",
      "train_y.shape = (40587, 6), valid_y.shape = (4337, 6)\n",
      "Start training CatBoost\n",
      "  0%|          | 0/6 [00:00<?, ?it/s]Target: X4_mean, R2: 0.765\n",
      " 17%|█▋        | 1/6 [05:08<25:43, 308.73s/it]Target: X11_mean, R2: 0.718\n",
      " 33%|███▎      | 2/6 [09:58<19:49, 297.42s/it]Target: X18_mean, R2: 0.809\n",
      " 50%|█████     | 3/6 [14:46<14:40, 293.44s/it]Target: X26_mean, R2: 0.408\n",
      " 67%|██████▋   | 4/6 [19:54<09:57, 298.97s/it]Target: X50_mean, R2: 0.696\n",
      " 83%|████████▎ | 5/6 [24:59<05:01, 301.20s/it]Target: X3112_mean, R2: 0.665\n",
      "100%|██████████| 6/6 [29:47<00:00, 297.97s/it]\n",
      "Mean R2: 0.677\n",
      "Start Testing\n"
     ]
    }
   ],
   "source": [
    "for fold, (train_idx, val_idx) in enumerate(kf.split(train_df)):\n",
    "    if fold not in CFG.fold_list:\n",
    "        continue\n",
    "\n",
    "    LOGGER.info(f'Fold {fold + 1}/{kf.n_splits}')\n",
    "    valid_scores = {target: -999 for target in CFG.target_cols}\n",
    "\n",
    "    if CFG.full_data_train:\n",
    "        train_subset = train_df\n",
    "    else:\n",
    "        train_subset = train_df.iloc[train_idx]\n",
    "    val_subset = train_df.iloc[val_idx]\n",
    "    LOGGER.info(f\"train_subset.shape = {train_subset.shape}, val_subset.shape = {val_subset.shape}\")\n",
    "\n",
    "    if CFG.use_clip:\n",
    "        for col in CFG.target_cols:\n",
    "            lower_quantile = train_subset[col].quantile(CFG.clip_min)\n",
    "            upper_quantile = train_subset[col].quantile(CFG.clip_max)\n",
    "            train_subset = train_subset[(train_subset[col] >= lower_quantile) & (train_subset[col] <= upper_quantile)]\n",
    "            after_clip_index = train_subset.index\n",
    "        LOGGER.info(f\"after clip, train_subset.shape = {train_subset.shape}, val_subset.shape = {val_subset.shape}\")\n",
    "    \n",
    "    if CFG.use_label_norm:\n",
    "        train_subset, minmax_dict = label_norm(train_subset, CFG.use_log10, CFG.label_norm_method, None)\n",
    "        val_subset, _  = label_norm(val_subset, CFG.use_log10, CFG.label_norm_method, minmax_dict)\n",
    "\n",
    "    if CFG.use_feature_norm:\n",
    "        # 特征归一化\n",
    "        train_subset, scaler = feature_norm(train_subset, feature_cols, None)\n",
    "        val_subset, _ = feature_norm(val_subset, feature_cols, scaler)\n",
    "    \n",
    "    # embedding\n",
    "    if CFG.full_data_train:\n",
    "        train_image_embeddings_subset = train_image_embeddings[list(set(train_subset.index))]\n",
    "    else:\n",
    "        new_train_idx = list(set(train_idx) & set(train_subset.index))\n",
    "        train_image_embeddings_subset = train_image_embeddings[new_train_idx]\n",
    "    valid_image_embeddings_subset = train_image_embeddings[val_idx]\n",
    "\n",
    "    # feature\n",
    "    train_features = train_subset[feature_cols].values\n",
    "    valid_features = val_subset[feature_cols].values\n",
    "    \n",
    "    # y\n",
    "    train_y = train_subset[CFG.target_cols].values\n",
    "    valid_y = val_subset[CFG.target_cols].values\n",
    "\n",
    "\n",
    "    # 简单的特征工程\n",
    "    first_n_poly_feats = CFG.first_n_poly_feats\n",
    "    train_features_mask_all = np.concatenate(\n",
    "        (PolynomialFeatures(2).fit_transform(train_features)[:, :first_n_poly_feats], train_image_embeddings_subset), axis=1\n",
    "    )\n",
    "    val_features_mask_all = np.concatenate(\n",
    "        (PolynomialFeatures(2).fit_transform(valid_features)[:, :first_n_poly_feats], valid_image_embeddings_subset), axis=1\n",
    "    )\n",
    "\n",
    "\n",
    "    train_features_mask_df = pd.DataFrame(train_features_mask_all)\n",
    "    train_features_mask_df['emb'] = list(train_image_embeddings_subset)\n",
    "\n",
    "    val_features_mask_df = pd.DataFrame(val_features_mask_all)\n",
    "    val_features_mask_df['emb'] = list(valid_image_embeddings_subset)\n",
    "\n",
    "    LOGGER.info(f\"train_features_mask_df.shape = {train_features_mask_df.shape}, val_features_mask_df.shape = {val_features_mask_df.shape}\")\n",
    "    LOGGER.info(f\"train_y.shape = {train_y.shape}, valid_y.shape = {valid_y.shape}\")\n",
    "\n",
    "    LOGGER.info(\"Start training CatBoost\")\n",
    "    models = {}\n",
    "    scores = {}\n",
    "    for i, col in tqdm(enumerate(CFG.target_cols), total=len(CFG.target_cols)):\n",
    "        y_curr = train_y[:, i]\n",
    "        y_curr_val = valid_y[:, i]\n",
    "        train_pool = Pool(train_features_mask_df, y_curr, embedding_features=['emb'])\n",
    "        val_pool = Pool(val_features_mask_df, y_curr_val, embedding_features=['emb'])\n",
    "        \n",
    "        # tried to tune these parameters but without real success \n",
    "        model = CatBoostRegressor(\n",
    "            iterations=CFG.iterations,\n",
    "            learning_rate=CFG.lr,\n",
    "            loss_function='RMSE', \n",
    "            verbose=0, \n",
    "            )\n",
    "        model.fit(train_pool)\n",
    "        models[col] = model\n",
    "        \n",
    "        y_curr_val_pred = model.predict(val_pool)\n",
    "        \n",
    "        r2_col = r2_score(y_curr_val, y_curr_val_pred)\n",
    "        scores[col] = r2_col\n",
    "        LOGGER.info(f'Target: {col}, R2: {r2_col:.3f}')\n",
    "        \n",
    "    # this val score somewhat correlates with submission score bit I didn't really bother\n",
    "    mean_r2 = np.mean(list(scores.values()))\n",
    "    LOGGER.info(f'Mean R2: {mean_r2:.3f}')\n",
    "\n",
    "\n",
    "    # test prediction\n",
    "    LOGGER.info(\"Start Testing\")\n",
    "    if CFG.use_label_norm:\n",
    "        test_df, _ = label_norm(test_df, CFG.use_log10, 'minmax', minmax_dict)\n",
    "    if CFG.use_feature_norm:\n",
    "        test_df, _ = feature_norm(test_df, feature_cols, scaler)\n",
    "\n",
    "    test_features = test_df[feature_cols].values\n",
    "\n",
    "    test_features_all = np.concatenate(\n",
    "        (PolynomialFeatures(2).fit_transform(test_features)[:, :first_n_poly_feats], test_image_embeddings), axis=1\n",
    "    )\n",
    "\n",
    "    test_features_mask_df = pd.DataFrame(test_features_all)\n",
    "    test_features_mask_df['emb'] = list(test_image_embeddings)\n",
    "\n",
    "    submission = pd.DataFrame({'id': test_df['id']})\n",
    "    submission[CFG.target_cols] = 0\n",
    "    submission.columns = submission.columns.str.replace('_mean', '')\n",
    "\n",
    "    for i, col in enumerate(CFG.target_cols):\n",
    "        test_pool = Pool(test_features_mask_df, embedding_features=['emb'])\n",
    "        col_pred = models[col].predict(test_pool)\n",
    "        submission[col.replace('_mean', '')] = col_pred\n",
    "\n",
    "    submission.to_csv(f'{output_dir}/sub_{cur_time_abbr}_f{fold}_cv{mean_r2:.3f}.csv', index=False)\n",
    "    submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(CFG.fold_list) > 1:\n",
    "    from glob import glob\n",
    "    # Ensembling\n",
    "    LOGGER.info(\"Start Ensembling\")\n",
    "    all_submissions = []\n",
    "    for path in glob(f'{output_dir}/*.csv'):\n",
    "        all_submissions.append(pd.read_csv(path))\n",
    "\n",
    "    LOGGER.info(f\"Total {len(all_submissions)} submissions\")\n",
    "    # mean\n",
    "    ens_submission = all_submissions[0].copy()\n",
    "    ens_submission[CFG.sub_cols] = 0\n",
    "    for sub in all_submissions:\n",
    "        ens_submission[CFG.sub_cols] += sub[CFG.sub_cols] / len(all_submissions)\n",
    "    ens_submission.to_csv(f'{output_dir}/sub_{cur_time_abbr}_ens{len(all_submissions)}.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from glob import glob\n",
    "# csv_files = [\n",
    "#     './output/08121650_catboost/sub_08121513_f0_cv0.474.csv',\n",
    "#     './output/08121650_catboost/sub_08121513_f2_cv0.466.csv',\n",
    "#     './output/08121650_catboost/sub_08121650_f8_cv0.479.csv',\n",
    "#     './output/08121650_catboost/sub_08121650_f9_cv0.467.csv',\n",
    "#  ]\n",
    "# all_submissions = []\n",
    "# for path in csv_files:\n",
    "#     all_submissions.append(pd.read_csv(path))\n",
    "# print(f\"Total {len(all_submissions)} submissions\")\n",
    "\n",
    "# # mean\n",
    "# ens_submission = all_submissions[0].copy()\n",
    "# ens_submission[CFG.sub_cols] = 0\n",
    "# for sub in all_submissions:\n",
    "#     ens_submission[CFG.sub_cols] += sub[CFG.sub_cols] / len(all_submissions)\n",
    "# ens_submission.to_csv(f'{output_dir}/sub_{cur_time_abbr}_ens4best.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total 3 submissions\n"
     ]
    }
   ],
   "source": [
    "# from glob import glob\n",
    "# csv_files = [\n",
    "#     './output/08122049_catboost/sub_08122049_f0_cv0.689.csv', # seed 42\n",
    "#     './output/08122124_catboost/sub_08122124_f0_cv0.692.csv', # seed 43\n",
    "#     './output/08122154_catboost/sub_08122154_f0_cv0.677.csv', # seed 44\n",
    "\n",
    "#  ]\n",
    "# all_submissions = []\n",
    "# for path in csv_files:\n",
    "#     all_submissions.append(pd.read_csv(path))\n",
    "# print(f\"Total {len(all_submissions)} submissions\")\n",
    "\n",
    "# # mean\n",
    "# ens_submission = all_submissions[0].copy()\n",
    "# ens_submission[CFG.sub_cols] = 0\n",
    "# for sub in all_submissions:\n",
    "#     ens_submission[CFG.sub_cols] += sub[CFG.sub_cols] / len(all_submissions)\n",
    "# ens_submission.to_csv(f'{output_dir}/sub_{cur_time_abbr}_ens3seed.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
